{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### \n",
    "The dataset attached contains the data of 160 different bags associated with ABC industries. The bags have certain attributes which are described below:\n",
    "\n",
    "*) Height – The height of the bag\n",
    "\n",
    "*) Width – The width of the bag\n",
    "\n",
    "*) Length – The length of the bag\n",
    "\n",
    "*) Weight – The weight the bag can carry\n",
    "\n",
    "*) Weight1 – Weight the bag can carry after expansion\n",
    "\n",
    " \n",
    "\n",
    "The company now wants to predict the cost they should set for a new variant of these kinds of bags based on the attributes below. As a result, they want you to build a prediction model which can correctly set the cost of the bag provided the attributes are given. The task involves the following things:\n",
    "\n",
    "> Analyse the dataset and do EDA(Exploratory Data Analysis)\n",
    "\n",
    "> Plotting of various graphs & correlations –\n",
    "\n",
    "> Model Building using Multiple Linear Regression – [SGD, Mini Batch, Gradient Descent, Normal SK-Learn library]\n",
    "\n",
    "> Calculating the R squared, RMSE and MSE for the model \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "sn.set(rc={'figure.figsize':(10,5)})\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file into a dataframe\n",
    "dataset_df = pd.read_csv(\"Data_miniproject.csv\")\n",
    "\n",
    "# Print Dataframe information and description\n",
    "print(\"\\n Dataset_df.info()  \\n\",dataset_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 CORRELATION AND HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Correlation\n",
    "dataset_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot correlation using heatmap\n",
    "sn.heatmap(dataset_df.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Correlation value lies between -1 and +1 . The sign indicates if it is positive or negative correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferences:- \n",
    "1. Weight has Strong Correlation with target variable(cost)\n",
    "2. Height and width have positive correlation with cost\n",
    " \n",
    "  Collinerity among independent variables\n",
    "3. Weight1 and Length have strong correlation with weight, so they can be removed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "dataset_df.drop(['Weight1','Length'],inplace=True,axis=1)\n",
    "dataset_df.columns\n",
    "dataset_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 CLEAN THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the row with any column with 0 or NAN value\n",
    "dataset_df=dataset_df[dataset_df!=0].dropna()\n",
    "dataset_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 FIND AND REMOVE THE OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the outliers using boxplot\n",
    "box = plt.boxplot(dataset_df['Cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain  the minimum and maximum cost\n",
    "min_max=[item.get_ydata()[0] for item in box['caps']]\n",
    "Cost_max = min_max[1] # maximum value of cost\n",
    "\n",
    "# Ouliers identified - obtain their index and remove the outliers\n",
    "index = dataset_df.index[dataset_df['Cost'] > Cost_max] # Identify outliers\n",
    "dataset_df = dataset_df.drop(labels =index,axis=0) # Remove outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.distplot(dataset_df['Cost']) #distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.lineplot(x='Weight',y='Cost',data=dataset_df) #line plot to get condensed view of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.lineplot(x='Height',y='Cost',data=dataset_df) #line plot to get condensed view of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dataset_df['Cost'],bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Standardizing/Scaling X and Y\n",
    "Stardardization is the process of bringing all features/variables into one single scale(normalized scale). This can be done by substracting mean from the values and dividing by the standard deviation of the feature or variable.StandardScaler available in sklearn.preprocessing package provides this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_df.iloc[:, 1:4].values \n",
    "y = dataset_df.iloc[:, 0].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaler = StandardScaler()\n",
    "X_scaled = X_scaler.fit_transform(X) #Standardize all the feature columns\n",
    "\n",
    "#Standarding y explicitly by substracting mean and dividing by sandard deviation\n",
    "y_scaled = (y-y.mean())/y.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing Gradient Descent Algorithm\n",
    "Method 1 : Method to Randomly initialize bias and weights\n",
    "\n",
    "Method 2 : Method to calculate the predicted value of y ,y given the bias(W0) and weights(W1,W2,W3,...Wn)\n",
    "\n",
    "Method 3 : Method to calulate the cost function from the predicted and actual value of y\n",
    "\n",
    "Method 4 : Method to calculate the gradients and adjust the bias and weights \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1 : RANDOM INITIALIZATION OF BIAS(W0) AND WEIGHTS(W1,W2,W3,....Wn)\n",
    "import random\n",
    "def initialize(dim) :\n",
    "    #For reproducible results , seed it to 42\n",
    "    np.random.seed(seed=42)\n",
    "    random.seed(42)\n",
    "    b = random.random() #Initialize the bias\n",
    "    w = np.random.rand(dim) # Initialize the weights\n",
    "    return b,w\n",
    "\n",
    "#Method 2 : PREDICT y VALUES FROM BIAS AND WEIGHTS\n",
    "#Inputs :  \n",
    "#    b(W0)    : bias\n",
    "#    Weights  : (W1,W2,W3,...,Wn)\n",
    "#    X        : the input matrix\n",
    "\n",
    "def predict_y(b,w,X):\n",
    " return b+np.matmul(X,w)\n",
    "\n",
    "#Method 3  : CALCULATE THE COST FUNCTION - MSE\n",
    "#Inputs : \n",
    "# y     - Actual values of y\n",
    "# y_hat - predicted value of y\n",
    "\n",
    "def get_cost(y,y_hat):\n",
    "    # Calculate residuals : - difference between actual and predicted values \n",
    "    y_residue = y-y_hat\n",
    "    return np.sum(np.matmul(y_residue.T,y_residue))/len(y_residue)\n",
    "\n",
    "\n",
    "#Method 4:  METHOD TO UPDATE BIAS AND WEIGHTS\n",
    "#Parameters passed : \n",
    "# x,y : - Input and Output variables\n",
    "# y_hat : - Predicted vaklue with the current bias and weights\n",
    "# b-0 , w_0 :  current Bias and Weights\n",
    "# learning_rate :  Learning rateto adjust the update step\n",
    "\n",
    "def update_beta(x,y,y_hat,b_0,w_0,learning_rate):\n",
    "    #Gradient of bias\n",
    "    db = (np.sum(y_hat-y)*2)/len(y)\n",
    "    #Gradient of Weights\n",
    "    dw = (np.dot((y_hat-y),x))/len(y)\n",
    "    #Update bias and beta\n",
    "    b_1 = b_0 - learning_rate * db\n",
    "    w_1 = w_0 - learning_rate * dw\n",
    "    \n",
    "    # return new bias and beta values\n",
    "    return b_1,w_1\n",
    "\n",
    "\n",
    "#Final Gradient Descent Algo\n",
    "def run_gradient_descent(X,y,learningRate=.01,numIterations=100):\n",
    "    #Initialize the bias and weights\n",
    "    b,w = initialize(X.shape[1]) \n",
    "    iter_num = 0;\n",
    "    \n",
    "    #gd_iterations_df keeps track of cost \n",
    "    gd_iterations_df =  pd.DataFrame(columns=['iterations','cost','b(slope)','w(const)'])\n",
    "    result_idx = 0\n",
    "    prev_cost = cost = 0\n",
    "    \n",
    "    \n",
    "    #Run the iterations in loop\n",
    "    for each_iter in range(numIterations):\n",
    "        y_hat = predict_y(b,w,X) # Calculate the predicted value of y\\\n",
    "        cost = get_cost(y,y_hat) # Calculate the cost\n",
    "        if(prev_cost < cost and iter_num != 0 ):\n",
    "            print(\"current cost is greater then prev cost,pt of maxima crossed @ iteration :\",iter_num)\n",
    "            print(\"Prev_cost:\",prev_cost,\"Current cost:\",cost)\n",
    "            break\n",
    "        prev_b = b # save the previous bias and weights\n",
    "        prev_w = w\n",
    "        #Update and calculate the new values of bias and weights\n",
    "        b,w = update_beta(X,y,y_hat,prev_b,prev_w,learningRate) \n",
    "        gd_iterations_df.loc[result_idx] = [iter_num,cost,b,w]\n",
    "        prev_cost = cost\n",
    "        result_idx = result_idx + 1\n",
    "        iter_num = iter_num + 1\n",
    "           \n",
    "        \n",
    "    print(\"\\n ==> Final cost :\",cost,\" \\n ==> Final Estimate of bias and weight(s):\", b,w)\n",
    "    #Return the final bias , weights and cost at the end\n",
    "    return gd_iterations_df,b,w\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start with 10000 iterations and a learning rate of 0.01.\n",
    "gradient1, b,w = run_gradient_descent(X_scaled,y_scaled,learningRate=.01,numIterations=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the below graph,it can be noticed that the cost is still reducing and has not reached the minimum point even after 5000 iterations.\n",
    "We can increase the learnig rate and verify if the cost is reaching a minimum point or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = gradient1[:100]\n",
    "plt.plot(plot1['iterations'],plot1['cost'],label=\"LearningRate=.01\",color ='blue',linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost(MSE)\")\n",
    "plt.title(\"Cost(MSE) vs Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Increase the learning rate to 0.05.\n",
    "gradient2, b,w = run_gradient_descent(X_scaled,y_scaled,learningRate=.05,numIterations=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the below graph , we notice that with the increase in learning rate to .05 we reach the minimum point faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2 = gradient2[:100]\n",
    "plt.plot(plot2['iterations'],plot2['cost'],label=\"LearningRate=.01\",color ='blue',linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost(MSE)\")\n",
    "plt.title(\"Cost(MSE) vs Iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning rate =.05 ,after around 60 iterations the cost is flat so the remaining iterations are not needed or will not result in any further optimization. Let us zoom in till iteration 100 and see the curve .\n",
    "\n",
    "Also ,Now we plot the cost after every iteration for different learning rate parameters (alpha values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the cost function against iterations\n",
    "plt.plot(plot1['iterations'],plot1['cost'],label=\"LearningRate=.01\",color ='blue',linestyle='dashed')\n",
    "plt.plot(plot2['iterations'],plot2['cost'],label=\"LearningRate=.05\",color ='green',linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost vs Iterations for different learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot in above figure shows that the learning is faster for Learning rate0.05 compared to 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stochastic Gradient Descent\n",
    "Using Sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=10_000, tol=.001,eta0=1e-3)\n",
    "sgd_reg.fit(X_scaled,y_scaled)\n",
    "SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(sgd_reg.history['accuracy'], label='train')\n",
    "#pyplot.plot(sgd_reg.history['val_accuracy'], label='test')\n",
    "#pyplot.title('lrate='+str(lrate), pad=-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num of iterations :\",sgd_reg.n_iter_)\n",
    "print(\"\\nCoeffcient:\",sgd_reg.coef_)\n",
    "print(\"\\nIntercept :\" ,sgd_reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSD_pred_y = sgd_reg.predict(X_scaled)\n",
    "print('Mean Absolute Error:%.2f'% metrics.mean_absolute_error(y_scaled, GSD_pred_y))  \n",
    "print('(MSE) Mean Squared Error:%.2f'% metrics.mean_squared_error(y_scaled,GSD_pred_y))  \n",
    "print('(RMSE) Root Mean Squared Error:%.2f'% np.sqrt(metrics.mean_squared_error(y_scaled,GSD_pred_y)))\n",
    "print('Variance score = %.2f' % metrics.r2_score(y_scaled, GSD_pred_y))\n",
    "print('Score',sgd_reg.score(X_scaled, GSD_pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent Mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_cost(theta,X,y):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/2*m) * np.sum(np.square(predictions-y))\n",
    "    return cost\n",
    "\n",
    "def minibatch_gradient_descent(X,y,theta,learning_rate=0.01, iterations=10, batch_size =10):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    n_batches = int(m/batch_size)\n",
    "    for it in range(iterations):\n",
    "        cost =0.0\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in range(0,m,batch_size):\n",
    "            X_i = X[i:i+batch_size]\n",
    "            y_i = y[i:i+batch_size]\n",
    "            X_i = np.c_[np.ones(len(X_i)),X_i]\n",
    "            theta = np.ones(4) #Parameters\n",
    "            theta = np.expand_dims(theta, axis=0)\n",
    "            theta = np.transpose(theta)\n",
    "            prediction = np.dot(X_i,theta)\n",
    "            theta = theta -(1/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n",
    "            cost += cal_cost(theta,X_i,y_i)\n",
    "        cost_history[it]  = cost\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = .05\n",
    "num_iterations = 10000\n",
    "batch_size = 5\n",
    "theta = np.random.randn(4, 1)\n",
    "theta,cost_history = minibatch_gradient_descent(X_scaled,y_scaled,theta,learningRate,num_iterations,batch_size)\n",
    "print('Mini Batch SGD')\n",
    "print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))\n",
    "print('(RMSE) Root Mean Squared Error:%.2f'% np.sqrt(cost_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(100),np.sqrt(cost_history[:100]),label='Learning Rate=.05',color ='blue',linestyle='dashed')\n",
    "#plt.plot(range(n_iter),cost_history,color ='blue',linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"theta\")\n",
    "plt.title(\"Cost(MSE) vs Iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data set into training data and test/validation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Simple Linear Regression to the Training dataset\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model, using training data set\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCoefficient(m)W1 :\",model.coef_)\n",
    "print(\"\\nIntercept(C)W0   :\",model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trained model to predict on test dataset\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print('Mean Absolute Error:%.2f'% metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:%.2f'% metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:%.2f'% np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction.\n",
    "# R^2 (coefficient of determination) regression score function.\n",
    "# Best possible score is 1.0, a\n",
    "# a constant model that always predicts the expected value of y, disregarding the input features, \n",
    "#would get a R^2 score of 0.0.\n",
    "\n",
    "print('Variance score = %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = sm.OLS(y_train, X_train).fit()\n",
    "model_1.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test-y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
